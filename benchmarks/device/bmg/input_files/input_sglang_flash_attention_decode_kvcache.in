# Flash attention decode (with kv-cache)
FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile512_h128_NonCausal_FixedLen --bm_name=attention_decode_kv_cache --batch=1 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=2048 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile512_h128_Causal_FixedLen --bm_name=attention_decode_kv_cache --batch=1 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=2048 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile512_h128_NonCausal_FixedLen --bm_name=attention_decode_kv_cache --batch=8 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile512_h128_Causal_FixedLen --bm_name=attention_decode_kv_cache --batch=8 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile512_h128_NonCausal_FixedLen --bm_name=attention_decode_kv_cache --batch=16 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile512_h128_Causal_FixedLen --bm_name=attention_decode_kv_cache --batch=16 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8

FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile512_h128_NonCausal_VarLen --bm_name=attention_decode_kv_cache --batch=1 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=2048 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile512_h128_Causal_VarLen --bm_name=attention_decode_kv_cache --batch=1 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=2048 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile512_h128_NonCausal_VarLen --bm_name=attention_decode_kv_cache --batch=8 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile512_h128_Causal_VarLen --bm_name=attention_decode_kv_cache --batch=8 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile512_h128_NonCausal_VarLen --bm_name=attention_decode_kv_cache --batch=16 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile512_h128_Causal_VarLen --bm_name=attention_decode_kv_cache --batch=16 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8

FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile1024_h128_NonCausal_FixedLen --bm_name=attention_decode_kv_cache --batch=1 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=2048 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile1024_h128_Causal_FixedLen --bm_name=attention_decode_kv_cache --batch=1 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=2048 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile1024_h128_NonCausal_FixedLen --bm_name=attention_decode_kv_cache --batch=8 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile1024_h128_Causal_FixedLen --bm_name=attention_decode_kv_cache --batch=8 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile1024_h128_NonCausal_FixedLen --bm_name=attention_decode_kv_cache --batch=16 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile1024_h128_Causal_FixedLen --bm_name=attention_decode_kv_cache --batch=16 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8

FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile1024_h128_NonCausal_VarLen --bm_name=attention_decode_kv_cache --batch=1 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=2048 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile1024_h128_Causal_VarLen --bm_name=attention_decode_kv_cache --batch=1 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=2048 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile1024_h128_NonCausal_VarLen --bm_name=attention_decode_kv_cache --batch=8 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile1024_h128_Causal_VarLen --bm_name=attention_decode_kv_cache --batch=8 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile1024_h128_NonCausal_VarLen --bm_name=attention_decode_kv_cache --batch=16 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeBF16BF16FP32FP32_RCR_NonPaged_KVTile1024_h128_Causal_VarLen --bm_name=attention_decode_kv_cache --batch=16 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8


#FP16 benchmarks
FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile512_h128_NonCausal_FixedLen --bm_name=attention_decode_kv_cache --batch=1 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=2048 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile512_h128_Causal_FixedLen --bm_name=attention_decode_kv_cache --batch=1 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=2048 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile512_h128_NonCausal_FixedLen --bm_name=attention_decode_kv_cache --batch=8 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile512_h128_Causal_FixedLen --bm_name=attention_decode_kv_cache --batch=8 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile512_h128_NonCausal_FixedLen --bm_name=attention_decode_kv_cache --batch=16 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile512_h128_Causal_FixedLen --bm_name=attention_decode_kv_cache --batch=16 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8

FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile512_h128_NonCausal_VarLen --bm_name=attention_decode_kv_cache --batch=1 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=2048 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile512_h128_Causal_VarLen --bm_name=attention_decode_kv_cache --batch=1 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=2048 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile512_h128_NonCausal_VarLen --bm_name=attention_decode_kv_cache --batch=8 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile512_h128_Causal_VarLen --bm_name=attention_decode_kv_cache --batch=8 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile512_h128_NonCausal_VarLen --bm_name=attention_decode_kv_cache --batch=16 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile512_h128_Causal_VarLen --bm_name=attention_decode_kv_cache --batch=16 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8


FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile1024_h128_NonCausal_FixedLen --bm_name=attention_decode_kv_cache --batch=1 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=2048 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile1024_h128_Causal_FixedLen --bm_name=attention_decode_kv_cache --batch=1 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=2048 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile1024_h128_NonCausal_FixedLen --bm_name=attention_decode_kv_cache --batch=8 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile1024_h128_Causal_FixedLen --bm_name=attention_decode_kv_cache --batch=8 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile1024_h128_NonCausal_FixedLen --bm_name=attention_decode_kv_cache --batch=16 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile1024_h128_Causal_FixedLen --bm_name=attention_decode_kv_cache --batch=16 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8

FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile1024_h128_NonCausal_VarLen --bm_name=attention_decode_kv_cache --batch=1 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=2048 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile1024_h128_Causal_VarLen --bm_name=attention_decode_kv_cache --batch=1 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=2048 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile1024_h128_NonCausal_VarLen --bm_name=attention_decode_kv_cache --batch=8 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile1024_h128_Causal_VarLen --bm_name=attention_decode_kv_cache --batch=8 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile1024_h128_NonCausal_VarLen --bm_name=attention_decode_kv_cache --batch=16 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
FMHADecodeFP16FP16FP32FP32_RCR_NonPaged_KVTile1024_h128_Causal_VarLen --bm_name=attention_decode_kv_cache --batch=16 --seq_len_qo=1 --num_heads_q=32 --head_size_qk=128 --head_size_vo=128 --seq_len_kv=1024 --seq_len_kv_cache=1024 --num_heads_kv=8
